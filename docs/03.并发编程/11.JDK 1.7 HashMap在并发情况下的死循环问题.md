---
title: JDK 1.7 HashMap在并发情况下的死循环问题
date: 2022-11-03 08:39:23
permalink: /pages/7fea8a/
categories: 
  - 并发编程
tags: 
  - null
author: 
  name: xugaoyi
  link: https://github.com/xugaoyi
---
### 问题

最近的几次面试中，我都问了是否了解 HashMap 在并发使用时可能发生死循环，导致 cpu100%，结果让我很意外，都表示不知道有这样的问题，让我意外的是面试者的工作年限都不短。

由于 HashMap 并非是线程安全的，所以在高并发的情况下必然会出现问题，这是一个普遍的问题，虽然网上分析的文章很多，还是觉得有必须写一篇文章，让关注我公众号的同学能够意识到这个问题，并了解这个死循环是如何产生的。

如果是在单线程下使用 HashMap，自然是没有问题的，如果后期由于代码优化，这段逻辑引入了多线程并发执行，在一个未知的时间点，会发现 CPU 占用 100%，居高不下，通过查看堆栈，你会惊讶的发现，线程都 Hang 在 hashMap 的 get() 方法上，服务重启之后，问题消失，过段时间可能又复现了。

这是为什么？

### 原因分析

在了解来龙去脉之前，我们先看看 HashMap 的数据结构。

在内部，HashMap 使用一个 Entry 数组保存 key、value 数据，当一对 key、value 被加入时，会通过一个 hash 算法得到数组的下标 index，算法很简单，根据 key 的 hash 值，对数组的大小取模 hash & (length-1)，并把结果插入数组该位置，如果该位置上已经有元素了，就说明存在 hash 冲突，这样会在 index 位置生成链表。

如果存在 hash 冲突，最惨的情况，就是所有元素都定位到同一个位置，形成一个长长的链表，这样 get 一个值时，最坏情况需要遍历所有节点，性能变成了 O(n)，所以元素的 hash 值算法和 HashMap 的初始化大小很重要。

当插入一个新的节点时，如果不存在相同的 key，则会判断当前内部元素是否已经达到阈值（默认是数组大小的 0.75），如果已经达到阈值，会对数组进行扩容，也会对链表中的元素进行 rehash。

### 实现

HashMap 的 put 方法实现：

1、判断 key 是否已经存在

```java
public V put(K key, V value) {
    if (key == null)
        return putForNullKey(value);
    int hash = hash(key);
    int i = indexFor(hash, table.length);
    // 如果key已经存在，则替换value，并返回旧值
    for (Entry<K,V> e = table[i]; e != null; e = e.next) {
        Object k;
        if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {
            V oldValue = e.value;
            e.value = value;
            e.recordAccess(this);
            return oldValue;
        }
    }
    modCount++;
    // key不存在，则插入新的元素
    addEntry(hash, key, value, i);
    return null;
}
```

2、检查容量是否达到阈值 threshold

```java
void addEntry(int hash, K key, V value, int bucketIndex) {
    if ((size >= threshold) && (null != table[bucketIndex])) {
        resize(2 * table.length);
        hash = (null != key) ? hash(key) : 0;
        bucketIndex = indexFor(hash, table.length);
    }
    createEntry(hash, key, value, bucketIndex);
}
```

如果元素个数已经达到阈值，则扩容，并把原来的元素移动过去。

3、扩容实现

```java
void resize(int newCapacity) {
    Entry[] oldTable = table;
    int oldCapacity = oldTable.length;
    ...

    Entry[] newTable = new Entry[newCapacity];
    ...
    transfer(newTable, rehash);
    table = newTable;
    threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);
}
```

这里会新建一个更大的数组，并通过 transfer 方法，移动元素。

```java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```

移动的逻辑也很清晰，遍历原来 table 中每个位置的链表，并对每个元素进行重新 hash，在新的 newTable 找到归宿，并插入。

### 案例分析

假设 HashMap 初始化大小为 4，插入个 3 节点，不巧的是，这 3 个节点都 hash 到同一个位置，如果按照默认的负载因子的话，插入第 3 个节点就会扩容，为了验证效果，假设负载因子是 1.

```java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}


```

以上是节点移动的相关逻辑。

![](https://img.jssjqd.cn/202211030840235.png)

插入第 4 个节点时，发生 rehash，假设现在有两个线程同时进行，线程 1 和线程 2，两个线程都会新建新的数组。

![](https://img.jssjqd.cn/202211030840019.png)

假设 **线程 2** 在执行到`Entry<K,V> next = e.next;`之后，cpu 时间片用完了，这时变量 e 指向节点 a，变量 next 指向节点 b。

**线程 1** 继续执行，很不巧，a、b、c 节点 rehash 之后又是在同一个位置 7，开始移动节点

第一步，移动节点 a

![](https://img.jssjqd.cn/202211030840532.png)

第二步，移动节点 b

![](https://img.jssjqd.cn/202211030840343.png)

注意，这里的顺序是反过来的，继续移动节点 c

![](https://img.jssjqd.cn/202211030840432.png)

这个时候 **线程 1** 的时间片用完，内部的 table 还没有设置成新的 newTable， **线程 2** 开始执行，这时内部的引用关系如下：

![](https://img.jssjqd.cn/202211030840639.png)

这时，在 **线程 2** 中，变量 e 指向节点 a，变量 next 指向节点 b，开始执行循环体的剩余逻辑。

```java
Entry<K,V> next = e.next;
int i = indexFor(e.hash, newCapacity);
e.next = newTable[i];
newTable[i] = e;
e = next;
```

执行之后的引用关系如下图

![](https://img.jssjqd.cn/202211030841029.png)

执行后，变量 e 指向节点 b，因为 e 不是 null，则继续执行循环体，执行后的引用关系

![](https://img.jssjqd.cn/202211030841936.png)

变量 e 又重新指回节点 a，只能继续执行循环体，这里仔细分析下：  
1、执行完`Entry<K,V> next = e.next;`，目前节点 a 没有 next，所以变量 next 指向 null；  
2、`e.next = newTable[i];` 其中 newTable[i] 指向节点 b，那就是把 a 的 next 指向了节点 b，这样 a 和 b 就相互引用了，形成了一个环；  
3、`newTable[i] = e` 把节点 a 放到了数组 i 位置；  
4、`e = next;` 把变量 e 赋值为 null，因为第一步中变量 next 就是指向 null；

所以最终的引用关系是这样的：

![](https://img.jssjqd.cn/202211030841546.png)

节点 a 和 b 互相引用，形成了一个环，当在数组该位置 get 寻找对应的 key 时，就发生了死循环。

另外，如果线程 2 把 newTable 设置成到内部的 table，节点 c 的数据就丢了，看来还有数据遗失的问题。

### 总结

所以在并发的情况，发生扩容时，可能会产生循环链表，在执行 get 的时候，会触发死循环，引起 CPU 的 100% 问题，所以一定要避免在并发环境下使用 HashMap。

曾经有人把这个问题报给了 Sun，不过 Sun 不认为这是一个 bug，因为在 HashMap 本来就不支持多线程使用，要并发就用 ConcurrentHashmap。

END。 

 [原文链接](https://www.jianshu.com/p/1e9cf0ac07f4)